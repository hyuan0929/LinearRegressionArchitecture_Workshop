{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè° Data Streaming and Visualization Workshop\n",
    "## Use case: Manufacturing Robot predictive Maintenance\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùó Clarify the Problem\n",
    "\n",
    "Issue: Torque Tube Failure ‚Äì 480 Minutes of Downtime\n",
    "Root cause is the age of the equipment.\n",
    "Roadblocks: Options to monitor equipment health are limited.\n",
    "GAP: Lack of a tool to avoid reactive response to equipment breakdown.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. üß≠ Material Handling Operations\n",
    "![Image Description](./images/KawasakiMaterialsHandling.png)\n",
    "\n",
    "\n",
    "### ‚ö†Ô∏è What Goes Wrong Without ProperMaintenance\n",
    "![Image Description](./images/KawasakiFailureCondition.png)\n",
    "\n",
    "### 2. üìâ Robot Controller Hardware Configuration\n",
    "![Image Description](./images/KawasakiASTerminalControl.png)\n",
    "\n",
    "### 3. üìâ Collecting Data from the Robot Controller\n",
    "![Image Description](./images/ASATerminalTelnetDataCollect.png)\n",
    "\n",
    "### 4. üß† Predictive Maintenance Use Case\n",
    "![Image Description](./images/FailurePredictionUseCase.png)\n",
    "\n",
    "### 5. ‚úÖ Predictive Maintenance (PM) Architecture\n",
    "![Image Description](./images/PM_Architecture.png)\n",
    "\n",
    "### 6. ‚û°Ô∏è PM Dashboard Design.\n",
    "![Image Description](./images/PM_SampleDashboard.png)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Predictive Maintenance Dashboard** application, the visibility tool of an Anomaly Detection and response management workflow in a manufacturing facility.\n",
    "\n",
    "### üë• Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "## üîß Workshop Tasks Overview\n",
    "\n",
    "1. **Streaming and Collection of robot operational data**\n",
    "2. **Persisting the data in a Relational Database**\n",
    "3. **Running a live dashboard to track robot hardware performance**\n",
    "4. **Implementing the foundations of a Predictive Analytics Module**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Learning Objectives\n",
    "- Implement a **Dynamic Dashboard** using real-world data from one or more operational robots.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## üß© Workshop Structure (180 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(40 min)* ‚Äì Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(120 min)* ‚Äì Collection, Persistence, and Dashboard coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(10 min)* ‚Äì Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(5 min)* ‚Äì Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: CSCN8010 - Data Stream Visualization Workshop, Team #_____.\n",
    "6. **Push to the course GitHub** *(automated)* ‚Äì Find the last code cell in this notebook, update your team number, and run the cell. It will push your notebook to the repo at `submissions/team#`\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `DataStreamVisualization_workshop.ipynb` with:\n",
    "  - Demo code: Data Streaming and Collection, DB Persistence, Visualization, and Predictive Analytics placeholder module.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Use case description and problem definition\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `DataStreamVisualization_workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Step 1: Simulate the Telnet connection before Document Collection starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üó£ Instructor Talking Point:\n",
    "> We begin by simulating the connection to N materials-handling robots as a set $ R = \\{r_1, r_2, \\dots, r_x\\}$, where $ x \\in \\mathbb{N} \\mid 1 \\leq x \\leq 100 $. \n",
    "\n",
    "To build a data collection mechanism, a simulation of a real **Streaming Data Collection System**. The simulation must open a CSV file with one day's worth of streaming data, then read one record, and concurrently (1) insert the data into a database table and (2) pushing the data into a dashboard.\n",
    "\n",
    "### üîß Your Task:\n",
    "- Open the provided CSV file and stream it to memory in a Pandas Data Frame.\n",
    "- Initialize a database on a cloud-based service like https://neon.tech/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Setting Up the Virtual Environment\n",
    "\n",
    "Before running the data simulation and analysis scripts, we need to set up a **Python virtual environment** and install the required packages.  \n",
    "This ensures a clean and consistent environment, avoiding conflicts with system Python packages.\n",
    "\n",
    "Run the following instructions:\n",
    "\n",
    "python -m venv .venv.\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Link to database\n",
    "conn_str = 'postgresql://neondb_owner:npg_Sh8bV3HjZvkd@ep-plain-scene-ahmzh8by-pooler.c-3.us-east-1.aws.neon.tech/neondb?sslmode=require'\n",
    "\n",
    "# Read .csv document\n",
    "file_path = r'data/RMBR4-2_export_test.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data Preprocessing\n",
    "df.columns = [col.lower().replace(' ', '_').replace('#', '') for col in df.columns]\n",
    "df.rename(columns={'time': 'recorded_at'}, inplace=True)\n",
    "cols_to_drop = ['axis_9', 'axis_10', 'axis_11', 'axis_12', 'axis_13', 'axis_14']\n",
    "df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "\n",
    "# Connect to the Database Engine\n",
    "try:\n",
    "    engine = create_engine(conn_str)\n",
    "    \n",
    "    print(\"Connecting to Neon and uploading data, please wait...\")\n",
    "    \n",
    "    # 5. Data Ingestion\n",
    "    # 'replace': Drops the table if it exists and creates a new one.\n",
    "    # 'append': Adds new rows to the existing table. \n",
    "    # Use 'append' if you have manually created the table with a specific schema.\n",
    "    df.to_sql('robot_1', engine, if_exists='replace', index=False)\n",
    "    \n",
    "    print(\"Success! Data has been imported in Neon.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Step 2: Simulate the data stream from robot controllers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó£ Instructor Talking Point:\n",
    "> Simulate the streaming data collection by opening the provided CSV file, and simulate ths stream of robot performance data as if it were coming from controllers, a single reading every 2 seconds.\n",
    "\n",
    "### üîß Your Task:\n",
    "- For each record read from the CSV file:\n",
    "    - Open a connection to the database\n",
    "    - Insert into a table in the database\n",
    "    - Plot on a chart (refresh it)\n",
    "    - Close the connection to the database\n",
    "\n",
    "- Implement the simuation module as an Object-Oriented Python script called `StreamingSimulator`.\n",
    "- Instantiate an object called `ss` in a Jupyter Notebook.\n",
    "- Invoke a method called `nextDataPoint` to load a record from the CSV document into a Data Frame for processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Class Explanation: `StreamingSimulator`\n",
    "\n",
    "**Purpose:**  \n",
    "Simulate a robot sending performance data in real-time, using a CSV file as the data source.\n",
    "\n",
    "**Key Features:**\n",
    "- Loads the entire CSV into memory using `pandas`.\n",
    "- Preprocesses column names to ensure database compatibility.\n",
    "- Maintains an internal pointer to the ‚Äúcurrent record‚Äù.\n",
    "- Each call to `nextDataPoint()`:\n",
    "  1. Reads the next record from the DataFrame.\n",
    "  2. Inserts it into the database.\n",
    "  3. Updates a real-time plot.\n",
    "  4. Waits for a configurable delay (e.g., 2 seconds) to simulate streaming.\n",
    "- Interactive plotting using `matplotlib` (`plt.ion()`).\n",
    "\n",
    "**Benefits of using a class:**\n",
    "- Encapsulates all data and functions in one reusable object.\n",
    "- Easy to extend for multiple robots or multiple data streams.\n",
    "- Provides a clean interface: just call `ss.nextDataPoint()` repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "class StreamingSimulator:\n",
    "    \"\"\"\n",
    "    Simulates streaming robot telemetry data from a CSV file.\n",
    "\n",
    "    Each call to nextDataPoint():\n",
    "        1) Opens a database connection\n",
    "        2) Inserts one record into a database table\n",
    "        3) Refreshes the chart\n",
    "        4) Closes the database connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, db_conn_str, table_name=\"robot_stream\", delay=2, max_xticks=50):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        csv_file (str): Relative path to the CSV file\n",
    "        db_conn_str (str): SQLAlchemy PostgreSQL connection string\n",
    "        table_name (str): Database table name to insert streaming data\n",
    "        delay (int/float): Seconds to wait between records (stream rate)\n",
    "        max_xticks (int): Maximum number of time labels on the X-axis\n",
    "        \"\"\"\n",
    "        # Load CSV into memory\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Standardize column names: lowercase + remove spaces + remove '#'\n",
    "        self.df.columns = [\n",
    "            col.lower().strip().replace(\" \", \"_\").replace(\"#\", \"\")\n",
    "            for col in self.df.columns\n",
    "        ]\n",
    "\n",
    "        # Rename time column to recorded_at if needed\n",
    "        if \"time\" in self.df.columns:\n",
    "            self.df.rename(columns={\"time\": \"recorded_at\"}, inplace=True)\n",
    "\n",
    "        # Convert timestamp column to datetime for better plotting\n",
    "        self.df[\"recorded_at\"] = pd.to_datetime(self.df[\"recorded_at\"])\n",
    "\n",
    "        # Detect axis columns (axis_1 to axis_8 only)\n",
    "        self.axis_cols = [\n",
    "            col for col in self.df.columns\n",
    "            if col.startswith(\"axis_\") and col.split(\"_\")[1].isdigit() and 1 <= int(col.split(\"_\")[1]) <= 8\n",
    "        ]\n",
    "        # Sort numerically (axis_1, axis_2, ..., axis_8)\n",
    "        self.axis_cols = sorted(self.axis_cols, key=lambda x: int(x.split(\"_\")[1]))\n",
    "\n",
    "        # Store configs\n",
    "        self.db_conn_str = db_conn_str\n",
    "        self.table_name = table_name\n",
    "        self.delay = delay\n",
    "        self.max_xticks = max_xticks\n",
    "        self.current_index = 0\n",
    "\n",
    "        # Create SQLAlchemy engine (connection opened/closed per record)\n",
    "        self.engine = create_engine(self.db_conn_str)\n",
    "\n",
    "        # Plot initialization\n",
    "        plt.ion()\n",
    "        self.fig, self.ax = plt.subplots(figsize=(12, 7))\n",
    "        self.x_data = []\n",
    "        self.y_data_dict = {col: [] for col in self.axis_cols}\n",
    "\n",
    "        print(f\"Loaded CSV: {csv_file}\")\n",
    "        print(f\"Detected Y-axis columns: {self.axis_cols}\")\n",
    "\n",
    "    def nextDataPoint(self):\n",
    "        \"\"\"\n",
    "        Loads one record from the CSV into a DataFrame row,\n",
    "        inserts it into the database, and refreshes the chart.\n",
    "        \"\"\"\n",
    "        # Stop condition\n",
    "        if self.current_index >= len(self.df):\n",
    "            print(\"All data points have been streamed.\")\n",
    "            return None\n",
    "\n",
    "        # Read the next row as a DataFrame\n",
    "        row = self.df.iloc[[self.current_index]]\n",
    "\n",
    "        # 1) Open connection -> 2) Insert record -> 4) Close connection\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                row.to_sql(self.table_name, conn, if_exists=\"append\", index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Database insert failed at index {self.current_index}: {e}\")\n",
    "\n",
    "        # 3) Update plot\n",
    "        ts = row[\"recorded_at\"].values[0]\n",
    "        self.x_data.append(ts)\n",
    "\n",
    "        for col in self.axis_cols:\n",
    "            self.y_data_dict[col].append(row[col].values[0])\n",
    "\n",
    "        self.ax.clear()\n",
    "\n",
    "        # Plot all axes\n",
    "        for col in self.axis_cols:\n",
    "            self.ax.plot(self.x_data, self.y_data_dict[col], label=col, linewidth=1)\n",
    "\n",
    "        # Chart formatting\n",
    "        self.ax.set_title(f\"Streaming Robot Axis Data ({self.current_index + 1}/{len(self.df)})\")\n",
    "        self.ax.set_xlabel(\"recorded_at\")\n",
    "        self.ax.set_ylabel(\"Axis Values\")\n",
    "\n",
    "        # Format time display\n",
    "        self.ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "\n",
    "        # Limit the number of X-axis time labels\n",
    "        if len(self.x_data) > self.max_xticks:\n",
    "            step = max(1, len(self.x_data) // self.max_xticks)\n",
    "            self.ax.set_xticks(self.x_data[::step])\n",
    "\n",
    "        # Legend outside the plot area\n",
    "        self.ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=\"small\")\n",
    "\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Refresh the figure\n",
    "        self.fig.canvas.draw()\n",
    "        self.fig.canvas.flush_events()\n",
    "        plt.pause(0.05)\n",
    "\n",
    "        # Move to the next row and simulate streaming delay\n",
    "        self.current_index += 1\n",
    "        time.sleep(self.delay)\n",
    "\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the `StreamingSimulator` class, we can instantiate it and simulate streaming robot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Instantiate simulator\n",
    "ss = StreamingSimulator(\n",
    "    csv_file='data/RMBR4-2_export_test.csv',\n",
    "    db_conn_str=conn_str,\n",
    "    table_name='robot_1_stream',\n",
    "    delay=2\n",
    ")\n",
    "\n",
    "# Stream one point\n",
    "ss.nextDataPoint()\n",
    "# Stream all points\n",
    "while True:\n",
    "    result = ss.nextDataPoint()\n",
    "    if result is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Step 3: Find patterns in the data stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó£ Instructor Talking Point:\n",
    "> Now we document if there are any tendencies or patterns in the data. Do this in the context of the use case and its problem statement.\n",
    "\n",
    "### üîß Your Task:\n",
    "- Write Markdown to document the data source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Document the application's role in the business use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó£ Instructor Talking Point:\n",
    "> Document the state of the robot(s) after analyzing the data stream.\n",
    "\n",
    "### üîß Your Task:\n",
    "- Pinpoint **anomalies** and comment on whether they affect the state of the robots or not.\n",
    "- Identify and document **Maintenance Notification alerts** based on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Additional Challenge: display a chart to summarize the entire data set\n",
    "\n",
    "Read the entire data from the database and use it to plot a chart that summarizes the behavior of the robots based on their energy consumption.\n",
    "\n",
    "#### Sample solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Push your code to the course GitHub repository\n",
    "\n",
    "Update your team number and then run the code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
