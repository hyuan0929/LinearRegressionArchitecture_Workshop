{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¡ Data Streaming and Visualization Workshop\n",
    "## Use case: Manufacturing Robot predictive Maintenance\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Predictive Maintenance Dashboard** application, the visibility tool of an Anomaly Detection and response management workflow in a manufacturing facility.\n",
    "\n",
    "### ðŸ‘¥ Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "## ðŸ”§ Workshop Tasks Overview\n",
    "\n",
    "1. **Streaming and Collection of robot operational data**\n",
    "2. **Persisting the data in a Relational Database**\n",
    "3. **Running a live dashboard to track robot hardware performance**\n",
    "4. **Implementing the foundations of a Predictive Analytics Module**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Learning Objectives\n",
    "- Implement a **Dynamic Dashboard** using real-world data from one or more operational robots.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## ðŸ§© Workshop Structure (180 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(40 min)* â€“ Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(120 min)* â€“ Collection, Persistence, and Dashboard coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(10 min)* â€“ Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(5 min)* â€“ Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: CSCN8010 - Data Stream Visualization Workshop, Team #_____.\n",
    "6. **Push to the course GitHub** *(automated)* â€“ Find the last code cell in this notebook, update your team number, and run the cell. It will push your notebook to the repo at `submissions/team#`\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `DataStreamVisualization_workshop.ipynb` with:\n",
    "  - Demo code: Data Streaming and Collection, DB Persistence, Visualization, and Predictive Analytics placeholder module.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- âœ… `README.md` with:\n",
    "  - Use case description and problem definition\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `DataStreamVisualization_workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 1: Simulate the Telnet connection before Document Collection starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We begin by simulating the connection to N materials-handling robots as a set $ R = \\{r_1, r_2, \\dots, r_x\\}$, where $ x \\in \\mathbb{N} \\mid 1 \\leq x \\leq 100 $. \n",
    "\n",
    "To build a data collection mechanism, a simulation of a real **Streaming Data Collection System**. The simulation must open a CSV file with one day's worth of streaming data, then read one record, and concurrently (1) insert the data into a database table and (2) pushing the data into a dashboard.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Open the provided CSV file and stream it to memory in a Pandas Data Frame.\n",
    "- Initialize a database on a cloud-based service like https://neon.tech/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”§ Setting Up the Virtual Environment\n",
    "\n",
    "Before running the data simulation and analysis scripts, we need to set up a **Python virtual environment** and install the required packages.  \n",
    "This ensures a clean and consistent environment, avoiding conflicts with system Python packages.\n",
    "\n",
    "Run the following instructions:\n",
    "\n",
    "python -m venv .venv.\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to Neon\n",
    "Create a database on Neon\n",
    "\n",
    "Connect String: postgresql://neondb_owner:npg_Sh8bV3HjZvkd@ep-plain-scene-ahmzh8by-pooler.c-3.us-east-1.aws.neon.tech/neondb?sslmode=require\n",
    "\n",
    "Export local CSV file to the Neon database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from src.stream_db import make_stream_engine, init_stream_table_from_csv\n",
    "\n",
    "# 1) Ensure project root (if running inside notebooks/)\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "root = os.getcwd()\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "# 2) Load config\n",
    "with open(\"configs/experiment_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# 3) Read connection + table + csv path\n",
    "connstr = config[\"database\"][\"connstr\"]\n",
    "table_name = config[\"database\"][\"source_table\"]\n",
    "\n",
    "# Prefer stream.raw_csv, fallback to paths.raw_csv\n",
    "csv_path = (\n",
    "    config.get(\"stream\", {}).get(\"raw_csv\")\n",
    "    or config.get(\"paths\", {}).get(\"raw_csv\")\n",
    ")\n",
    "\n",
    "# 4) Create engine + init table ONLY if empty\n",
    "engine = make_stream_engine(connstr, connect_timeout=10)\n",
    "rows = init_stream_table_from_csv(engine, table_name, csv_path)\n",
    "\n",
    "print(f\"âœ… Neon table '{table_name}' is ready. Rows = {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 2: Simulate the data stream from robot controllers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—£ Instructor Talking Point:\n",
    "> Simulate the streaming data collection by opening the provided CSV file, and simulate ths stream of robot performance data as if it were coming from controllers, a single reading every 2 seconds.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- For each record read from the CSV file:\n",
    "    - Open a connection to the database\n",
    "    - Insert into a table in the database\n",
    "    - Plot on a chart (refresh it)\n",
    "    - Close the connection to the database\n",
    "\n",
    "- Implement the simuation module as an Object-Oriented Python script called `StreamingSimulator`.\n",
    "- Instantiate an object called `ss` in a Jupyter Notebook.\n",
    "- Invoke a method called `nextDataPoint` to load a record from the CSV document into a Data Frame for processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“ Class Explanation: `StreamingSimulator`\n",
    "\n",
    "**Purpose:**  \n",
    "Simulate a robot sending performance data in real-time, using a CSV file as the data source.\n",
    "\n",
    "**Key Features:**\n",
    "- Loads the entire CSV into memory using `pandas`.\n",
    "- Preprocesses column names to ensure database compatibility.\n",
    "- Maintains an internal pointer to the â€œcurrent recordâ€.\n",
    "- Each call to `nextDataPoint()`:\n",
    "  1. Reads the next record from the DataFrame.\n",
    "  2. Inserts it into the database.\n",
    "  3. Updates a real-time plot.\n",
    "  4. Waits for a configurable delay (e.g., 2 seconds) to simulate streaming.\n",
    "- Interactive plotting using `matplotlib` (`plt.ion()`).\n",
    "\n",
    "**Benefits of using a class:**\n",
    "- Encapsulates all data and functions in one reusable object.\n",
    "- Easy to extend for multiple robots or multiple data streams.\n",
    "- Provides a clean interface: just call `ss.nextDataPoint()` repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stream_dashboard(config: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Run streaming visualization + popup alerts.\n",
    "\n",
    "    Expected YAML keys:\n",
    "      database.connstr\n",
    "      paths.stream_threshold_csv   (or fallback to paths.threshold_csv)\n",
    "      stream.source_table\n",
    "      stream.raw_csv\n",
    "      stream.delay\n",
    "      stream.plot_window_len\n",
    "      stream.max_xticks\n",
    "      stream.work_threshold\n",
    "      stream.min_work_points\n",
    "      stream.alert_cooldown_sec\n",
    "      stream.init_from_csv_if_empty\n",
    "    \"\"\"\n",
    "    connstr = config[\"database\"][\"connstr\"]\n",
    "\n",
    "    # Stream section (with sane defaults)\n",
    "    stream_cfg = config.get(\"stream\", {})\n",
    "    source_table = str(stream_cfg.get(\"source_table\", config[\"database\"].get(\"source_table\", \"robot_data\")))\n",
    "    raw_csv = str(stream_cfg.get(\"raw_csv\", config[\"paths\"].get(\"raw_csv\", \"data/raw/RMBR4-2_export_test.csv\")))\n",
    "\n",
    "    delay = float(stream_cfg.get(\"delay\", 0.1))\n",
    "    window_len = int(stream_cfg.get(\"plot_window_len\", 200))\n",
    "    max_xticks = int(stream_cfg.get(\"max_xticks\", 50))\n",
    "\n",
    "    work_threshold = float(stream_cfg.get(\"work_threshold\", 0.2))\n",
    "    min_work_points = int(stream_cfg.get(\"min_work_points\", 10))\n",
    "    alert_cooldown_sec = float(stream_cfg.get(\"alert_cooldown_sec\", 10.0))\n",
    "\n",
    "    init_from_csv_if_empty = bool(stream_cfg.get(\"init_from_csv_if_empty\", True))\n",
    "\n",
    "    # Thresholds file (prefer stream_threshold_csv, fallback to threshold_csv)\n",
    "    thresholds_csv = (\n",
    "        config.get(\"paths\", {}).get(\"stream_threshold_csv\")\n",
    "        or config.get(\"paths\", {}).get(\"threshold_csv\")\n",
    "        or \"data/models/alert_thresholds_stream.csv\"\n",
    "    )\n",
    "\n",
    "    print(\"=== Data Stream Visualization (Neon -> Live Plot + Popup Alerts) ===\")\n",
    "    print(f\"DB table: {source_table}\")\n",
    "    print(f\"Raw CSV (for init if empty): {raw_csv}\")\n",
    "    print(f\"Stream thresholds CSV: {thresholds_csv}\")\n",
    "    print()\n",
    "\n",
    "    # 1) Make engine + optionally init table once\n",
    "    engine = make_stream_engine(connstr, connect_timeout=10)\n",
    "\n",
    "    if init_from_csv_if_empty:\n",
    "        try:\n",
    "            n = init_stream_table_from_csv(engine, source_table, raw_csv)\n",
    "            print(f\"âœ… Table '{source_table}' ready. Rows = {n}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not init table from CSV (continuing): {e}\\n\")\n",
    "\n",
    "    # 2) Load streaming thresholds\n",
    "    thr = read_stream_alert_thresholds(thresholds_csv)\n",
    "\n",
    "    # 3) Run streaming simulator\n",
    "    sim = StreamingSimulator(\n",
    "        db_conn_str=connstr,\n",
    "        config=StreamConfig(\n",
    "            source_table=source_table,\n",
    "            delay=delay,\n",
    "            window_len=window_len,\n",
    "            max_xticks=max_xticks,\n",
    "            work_threshold=work_threshold,\n",
    "            min_work_points=min_work_points,\n",
    "            alert_cooldown_sec=alert_cooldown_sec,\n",
    "        ),\n",
    "        alert_thresholds=thr,\n",
    "    )\n",
    "\n",
    "    # 4) Stream loop\n",
    "    while True:\n",
    "        row = sim.nextDataPoint()\n",
    "        if row is None:\n",
    "            break\n",
    "\n",
    "    print(\"\\nâœ… Streaming finished.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 3: Find patterns in the data stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—£ Instructor Talking Point:\n",
    "> Now we document if there are any tendencies or patterns in the data. Do this in the context of the use case and its problem statement.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Write Markdown to document the data source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Document the application's role in the business use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—£ Instructor Talking Point:\n",
    "> Document the state of the robot(s) after analyzing the data stream.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Pinpoint **anomalies** and comment on whether they affect the state of the robots or not.\n",
    "- Identify and document **Maintenance Notification alerts** based on the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Additional Challenge: display a chart to summarize the entire data set\n",
    "\n",
    "Read the entire data from the database and use it to plot a chart that summarizes the behavior of the robots based on their energy consumption.\n",
    "\n",
    "#### Sample solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Push your code to the course GitHub repository\n",
    "\n",
    "Update your team number and then run the code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
