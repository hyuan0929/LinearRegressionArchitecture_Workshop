{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1a1d70",
   "metadata": {},
   "source": [
    "# ðŸ¡ Linear Regression Architecture Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cb630",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the **Linear Regression Architecture Workshop**.  \n",
    "This workshop is designed for college-level students learning both:\n",
    "\n",
    "1. **Univariate Linear Regression** â€“ a foundational algorithm in Machine Learning, focusing on predicting continuous values from a single feature.  \n",
    "2. **Machine Learning Operations (MLOps)** â€“ design patterns and architectural considerations that make machine learning experiments reproducible, scalable, and production-ready.  \n",
    "\n",
    "We will use **real-world housing price data** from **California (USA)** and **Ontario (Canada)** as our case study.  \n",
    "The goal is to not only understand how Linear Regression works, but also how to **design and implement a machine learning project** from sourcing data â†’ building models â†’ structuring code â†’ preparing for deployment.  \n",
    "\n",
    "The workshop will be completed in **two 2-hour sessions**, with **homework assignments** to be completed before each class.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b35c90",
   "metadata": {},
   "source": [
    "## Workshop Structure\n",
    "\n",
    "### ðŸ“š Session 1 â€“ Univariate Linear Regression\n",
    "- **Lecture focus**: Mathematical intuition, model formulation, gradient descent, cost function, evaluation metrics.  \n",
    "- **Practical focus**: Implementing Univariate Linear Regression from scratch + using `scikit-learn`.  \n",
    "- **Homework before class**: Data sourcing (from CSV, APIs, and relational databases).  \n",
    "\n",
    "### âš™ï¸ Session 2 â€“ Machine Learning Operations (MLOps)\n",
    "- **Lecture focus**: Code modularity, reproducibility, experiment tracking, design patterns in ML architecture.  \n",
    "- **Practical focus**: Architecting the project with pipelines, config management, and modular scripts.  \n",
    "- **Homework before class**: Refactor previous Linear Regression code into modular, production-ready format.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44cb510",
   "metadata": {},
   "source": [
    "## Instructions for Students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716f1b2",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Before Session 1: Data Sourcing\n",
    "\n",
    "Your first task is to collect **housing price data** for California and Ontario.  \n",
    "You must experiment with **at least three different types of data sources**:\n",
    "\n",
    "1. **CSV Files**  \n",
    "   - Find open housing datasets (e.g., Kaggle, UCI ML Repository, government portals).  \n",
    "   - Example: [California Housing Dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).  \n",
    "   - Save datasets in `data/raw/` folder.  \n",
    "\n",
    "2. **Web Services (APIs)**  \n",
    "   - Explore free APIs offering housing, rental, or real-estate data.  \n",
    "   - Example APIs:  \n",
    "     - [Zillow (unofficial APIs exist, check docs)]  \n",
    "     - [Realtor.ca data endpoints]  \n",
    "     - [City of Toronto Open Data API](https://open.toronto.ca/)  \n",
    "     - [California State Open Data Portal](https://data.ca.gov/).  \n",
    "   - Use Python packages like `requests` or `httpx` to fetch data.  \n",
    "   - Save results into structured JSON or convert to DataFrames.  \n",
    "\n",
    "3. **Relational Databases**  \n",
    "   - Connect to a **PostgreSQL** or **MySQL** demo database.  \n",
    "   - Option 1: Use hosted databases with sample housing/economic data.  \n",
    "   - Option 2: Load CSVs into a local database (e.g., PostgreSQL with `psql` or SQLite for portability).  \n",
    "   - Connect from Python using `sqlalchemy` or `psycopg2`.  \n",
    "   - Run SQL queries to filter/select data.  \n",
    "\n",
    "ðŸ’¡ **Deliverable before Session 1**:  \n",
    "- A Jupyter Notebook that loads housing price data from all three sources (CSV, API, Database) and explores it with basic descriptive statistics and plots.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49cefb",
   "metadata": {},
   "source": [
    "### ðŸ”¹ During Session 1: Univariate Linear Regression Experiment\n",
    "\n",
    "1. **Define the Problem**  \n",
    "   - Select one feature (e.g., median income, number of rooms, lot size) to predict housing price.  \n",
    "\n",
    "2. **Preprocess Data**  \n",
    "   - Handle missing values.  \n",
    "   - Normalize/standardize features.  \n",
    "   - Split data into **train/test sets**.  \n",
    "\n",
    "3. **Model Implementation**  \n",
    "   - Implement Linear Regression **from scratch**:  \n",
    "     - Hypothesis function $ h_\\theta(x) = \\theta_0 + \\theta_1 x $  \n",
    "     - Cost function (MSE)  \n",
    "     - Gradient descent update rule  \n",
    "   - Implement Linear Regression **using scikit-learn** for comparison.  \n",
    "\n",
    "4. **Model Evaluation**  \n",
    "   - Compute RMSE, MAE, and $ R^2 $ score.  \n",
    "   - Visualize regression line vs. data points.  \n",
    "\n",
    "ðŸ’¡ **Deliverable during Session 1**:  \n",
    "- A working notebook with both a manual and `scikit-learn` Linear Regression implementation.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51def6",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "MeanValue and MaxValue was selected as the independent variable and WorkPeriod as the dependent variable.\n",
    "The number of rooms is a fundamental housing characteristic with a clear and interpretable relationship to price. Using this feature supports the assumptions of univariate linear regression and keeps the model simple and easy to explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3dbad5",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77561c09",
   "metadata": {},
   "source": [
    "Using the force values measured on each robot axis, I will analyze the robotâ€™s time-series data, which consists of work periods and rest periods. The goal is to segment the data into work/rest periods based on the force signal. Each identified work period is assigned a sequential index, and this work-period number is used as the independent variable. For each work period, the mean force and peak force within that period are computed as the dependent variables.\n",
    "\n",
    "Next, I group the work periods into detection intervals, where each interval contains 10 consecutive work periods. Within each detection interval, I run regression analysis on the dependent variables over the work-period index:\n",
    "\n",
    "If the mean force shows a statistically significant upward trend, it suggests the robot may be experiencing system aging or gradual degradation.\n",
    "\n",
    "If the peak force shows a statistically significant upward trend, it suggests the robot may be at risk of an urgent or imminent failure.\n",
    "\n",
    "The raw dataset is located at:\n",
    "data/raw/RMBR4-2_export_test.csv\n",
    "\n",
    "I will transform it into a summarized table with the following fields:\n",
    "\n",
    "work_period\n",
    "\n",
    "mean_value\n",
    "\n",
    "peak_value\n",
    "\n",
    "interval_start_time (start time of the first work period in the interval)\n",
    "\n",
    "interval_end_time (end time of the last work period in the interval)\n",
    "\n",
    "Finally, I will export the resulting table to:\n",
    "data/raw/RMBR4-2_export_test_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00a6706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/raw/RMBR4-2_export_test_1.csv\n",
      "    work_period  mean_value  peak_value                period_start_time  \\\n",
      "0             1    3.302866    8.908955 2022-10-17 12:19:22.005000+00:00   \n",
      "1             2    3.449832    6.300471 2022-10-17 12:20:56.771000+00:00   \n",
      "2             3    3.491427    6.707684 2022-10-17 12:21:46.588000+00:00   \n",
      "3             4    3.255471    5.874910 2022-10-17 12:22:45.266000+00:00   \n",
      "4             5    2.881057    6.632019 2022-10-17 12:23:49.839000+00:00   \n",
      "5             6    2.839395    5.545404 2022-10-17 12:24:39.488000+00:00   \n",
      "6             7    3.030460    5.872156 2022-10-17 12:25:49.703000+00:00   \n",
      "7             8    2.643483    5.146125 2022-10-17 12:26:43.029000+00:00   \n",
      "8             9    3.208116    7.015793 2022-10-17 12:34:07.501000+00:00   \n",
      "9            10    3.465327    5.787941 2022-10-17 12:34:58.709000+00:00   \n",
      "10           11    3.586541    8.039739 2022-10-17 12:35:51.838000+00:00   \n",
      "11           12    3.764583    7.634454 2022-10-17 12:36:48.315000+00:00   \n",
      "12           13    2.908206    6.261834 2022-10-17 12:38:03.164000+00:00   \n",
      "13           14    3.188224    6.185474 2022-10-17 12:44:50.740000+00:00   \n",
      "14           15    3.479667    6.995066 2022-10-17 12:45:46.216000+00:00   \n",
      "\n",
      "                    period_end_time  \n",
      "0  2022-10-17 12:19:55.136000+00:00  \n",
      "1  2022-10-17 12:21:31.056000+00:00  \n",
      "2  2022-10-17 12:22:21.050000+00:00  \n",
      "3  2022-10-17 12:23:17.894000+00:00  \n",
      "4  2022-10-17 12:24:22.332000+00:00  \n",
      "5  2022-10-17 12:25:11.977000+00:00  \n",
      "6  2022-10-17 12:26:24.206000+00:00  \n",
      "7  2022-10-17 12:33:48.840000+00:00  \n",
      "8  2022-10-17 12:34:41.785000+00:00  \n",
      "9  2022-10-17 12:35:31.262000+00:00  \n",
      "10 2022-10-17 12:36:25.716000+00:00  \n",
      "11 2022-10-17 12:37:23.086000+00:00  \n",
      "12 2022-10-17 12:38:32.099000+00:00  \n",
      "13 2022-10-17 12:45:25.360000+00:00  \n",
      "14 2022-10-17 12:46:19.107000+00:00  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "INPUT_PATH  = \"data/raw/RMBR4-2_export_test.csv\"\n",
    "OUTPUT_PATH = \"data/raw/RMBR4-2_export_test_1.csv\"\n",
    "\n",
    "SMOOTH_SECONDS      = 6.0\n",
    "MAX_GAP_MULTIPLIER  = 10.0\n",
    "MIN_GAP_SECONDS     = 6.0\n",
    "MIN_WORK_SECONDS    = 8.0\n",
    "THRESH_QUANTILE     = 0.65\n",
    "THRESH_SCALE        = 0.5\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def standardize_columns(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    frame = frame.copy()\n",
    "    frame.columns = (\n",
    "        frame.columns.str.strip()\n",
    "                     .str.lower()\n",
    "                     .str.replace(\" \", \"_\", regex=False)\n",
    "                     .str.replace(\"#\", \"\", regex=False)\n",
    "    )\n",
    "    return frame\n",
    "\n",
    "def pick_time_column(columns) -> str:\n",
    "    candidates = [\"time\", \"recorded_at\", \"timestamp\", \"datetime\"]\n",
    "    for c in candidates:\n",
    "        if c in columns:\n",
    "            return c\n",
    "    raise ValueError(f\"Cannot find a time column. Available columns: {list(columns)}\")\n",
    "\n",
    "def compute_overall_force(frame: pd.DataFrame, axis_cols: list[str]) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Overall force per record.\n",
    "    Default: mean(abs(axis_i)) across axes.\n",
    "    \"\"\"\n",
    "    return frame[axis_cols].abs().mean(axis=1)\n",
    "\n",
    "def merge_short_false_gaps(mask: np.ndarray, gap_n: int) -> np.ndarray:\n",
    "    out = mask.copy()\n",
    "    n = len(out)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if not out[i]:\n",
    "            j = i\n",
    "            while j < n and (not out[j]):\n",
    "                j += 1\n",
    "            left_true  = (i - 1 >= 0 and out[i - 1])\n",
    "            right_true = (j < n and out[j])\n",
    "            if left_true and right_true and (j - i) <= gap_n:\n",
    "                out[i:j] = True\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def remove_short_true_runs(mask: np.ndarray, min_n: int) -> np.ndarray:\n",
    "    out = mask.copy()\n",
    "    n = len(out)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if out[i]:\n",
    "            j = i\n",
    "            while j < n and out[j]:\n",
    "                j += 1\n",
    "            if (j - i) < min_n:\n",
    "                out[i:j] = False\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return out\n",
    "\n",
    "def label_true_runs(mask: np.ndarray) -> np.ndarray:\n",
    "    labels = np.zeros(len(mask), dtype=int)\n",
    "    run_id = 0\n",
    "    i = 0\n",
    "    n = len(mask)\n",
    "    while i < n:\n",
    "        if mask[i]:\n",
    "            run_id += 1\n",
    "            j = i\n",
    "            while j < n and mask[j]:\n",
    "                j += 1\n",
    "            labels[i:j] = run_id\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return labels\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_PATH}\")\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "df = standardize_columns(df)\n",
    "\n",
    "time_col = pick_time_column(df.columns)\n",
    "\n",
    "axis_cols = [c for c in df.columns if c.startswith(\"axis_\")]\n",
    "if not axis_cols:\n",
    "    raise ValueError(f\"Cannot find axis columns. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Drop axes that are entirely NaN, and sort\n",
    "axis_cols = [c for c in axis_cols if df[c].notna().any()]\n",
    "axis_cols = sorted(axis_cols, key=lambda x: int(x.split(\"_\")[1]))\n",
    "\n",
    "# Parse time and sort\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\", utc=True)\n",
    "df = df.dropna(subset=[time_col]).sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "# Estimate sampling interval\n",
    "dt_sec = df[time_col].diff().dt.total_seconds()\n",
    "median_dt = float(dt_sec.dropna().median()) if dt_sec.dropna().size else 1.0\n",
    "if not np.isfinite(median_dt) or median_dt <= 0:\n",
    "    median_dt = 1.0\n",
    "\n",
    "# Compute overall force\n",
    "force = compute_overall_force(df, axis_cols)\n",
    "\n",
    "# Smooth force\n",
    "win = max(3, int(np.ceil(SMOOTH_SECONDS / median_dt)))\n",
    "force_smooth = force.rolling(win, min_periods=1).mean()\n",
    "\n",
    "# Adaptive threshold\n",
    "qv = float(np.nanpercentile(force_smooth, THRESH_QUANTILE * 100))\n",
    "thr = 0.05 if qv <= 0 else THRESH_SCALE * qv\n",
    "\n",
    "is_work = (force_smooth > thr).to_numpy()\n",
    "\n",
    "# Break across big gaps\n",
    "max_gap_seconds = MAX_GAP_MULTIPLIER * median_dt\n",
    "big_gap = (dt_sec.fillna(0) > max_gap_seconds).to_numpy()\n",
    "is_work = is_work & (~big_gap)\n",
    "\n",
    "# Merge short rest gaps and remove short work segments\n",
    "gap_n = max(1, int(np.ceil(MIN_GAP_SECONDS / median_dt)))\n",
    "min_n = max(2, int(np.ceil(MIN_WORK_SECONDS / median_dt)))\n",
    "\n",
    "is_work = merge_short_false_gaps(is_work, gap_n=gap_n)\n",
    "is_work = remove_short_true_runs(is_work, min_n=min_n)\n",
    "\n",
    "# Label work periods\n",
    "work_labels = label_true_runs(is_work)\n",
    "df[\"work_period\"] = work_labels\n",
    "df[\"overall_force\"] = force\n",
    "\n",
    "# Keep only work rows\n",
    "df_work = df[df[\"work_period\"] > 0].copy()\n",
    "if df_work.empty:\n",
    "    raise ValueError(\"No work periods detected. Try lowering THRESH_SCALE or THRESH_QUANTILE.\")\n",
    "\n",
    "# Summarize per work period (period-level times)\n",
    "summary = (\n",
    "    df_work.groupby(\"work_period\")\n",
    "           .agg(\n",
    "               mean_value=(\"overall_force\", \"mean\"),\n",
    "               peak_value=(\"overall_force\", \"max\"),\n",
    "               period_start_time=(time_col, \"min\"),\n",
    "               period_end_time=(time_col, \"max\"),\n",
    "           )\n",
    "           .reset_index()\n",
    ")\n",
    "\n",
    "# Save\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "summary.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved: {OUTPUT_PATH}\")\n",
    "print(summary.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23c0cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/processed\\RMBR4-2_export_processed.csv\n",
      "Train size: 686 | Test size: 172\n",
      "    work_period  mean_value  peak_value                period_start_time  \\\n",
      "0             1    3.302866    8.908955 2022-10-17 12:19:22.005000+00:00   \n",
      "1             2    3.449832    6.300471 2022-10-17 12:20:56.771000+00:00   \n",
      "2             3    3.491427    6.707684 2022-10-17 12:21:46.588000+00:00   \n",
      "3             4    3.255471    5.874910 2022-10-17 12:22:45.266000+00:00   \n",
      "4             5    2.881057    6.632019 2022-10-17 12:23:49.839000+00:00   \n",
      "5             6    2.839395    5.545404 2022-10-17 12:24:39.488000+00:00   \n",
      "6             7    3.030460    5.872156 2022-10-17 12:25:49.703000+00:00   \n",
      "7             8    2.643483    5.146125 2022-10-17 12:26:43.029000+00:00   \n",
      "8             9    3.208116    7.015793 2022-10-17 12:34:07.501000+00:00   \n",
      "9            10    3.465327    5.787941 2022-10-17 12:34:58.709000+00:00   \n",
      "10           11    3.586541    8.039739 2022-10-17 12:35:51.838000+00:00   \n",
      "11           12    3.764583    7.634454 2022-10-17 12:36:48.315000+00:00   \n",
      "12           13    2.908206    6.261834 2022-10-17 12:38:03.164000+00:00   \n",
      "13           14    3.188224    6.185474 2022-10-17 12:44:50.740000+00:00   \n",
      "14           15    3.479667    6.995066 2022-10-17 12:45:46.216000+00:00   \n",
      "\n",
      "                    period_end_time  interval_id  mean_value_z  peak_value_z  \\\n",
      "0  2022-10-17 12:19:55.136000+00:00            1      0.090353      1.422333   \n",
      "1  2022-10-17 12:21:31.056000+00:00            1      0.570955     -0.652364   \n",
      "2  2022-10-17 12:22:21.050000+00:00            1      0.706980     -0.328481   \n",
      "3  2022-10-17 12:23:17.894000+00:00            1     -0.064637     -0.990840   \n",
      "4  2022-10-17 12:24:22.332000+00:00            1     -1.289038     -0.388662   \n",
      "5  2022-10-17 12:25:11.977000+00:00            1     -1.425282     -1.252918   \n",
      "6  2022-10-17 12:26:24.206000+00:00            1     -0.800464     -0.993031   \n",
      "7  2022-10-17 12:33:48.840000+00:00            1     -2.065949     -1.570491   \n",
      "8  2022-10-17 12:34:41.785000+00:00            1     -0.219497     -0.083422   \n",
      "9  2022-10-17 12:35:31.262000+00:00            1      0.621628     -1.060012   \n",
      "10 2022-10-17 12:36:25.716000+00:00            2      1.018018      0.730989   \n",
      "11 2022-10-17 12:37:23.086000+00:00            2      1.600248      0.408639   \n",
      "12 2022-10-17 12:38:32.099000+00:00            2     -1.200258     -0.683095   \n",
      "13 2022-10-17 12:45:25.360000+00:00            2     -0.284548     -0.743829   \n",
      "14 2022-10-17 12:46:19.107000+00:00            2      0.668521     -0.099907   \n",
      "\n",
      "    split  \n",
      "0   train  \n",
      "1   train  \n",
      "2   train  \n",
      "3   train  \n",
      "4   train  \n",
      "5   train  \n",
      "6   train  \n",
      "7   train  \n",
      "8   train  \n",
      "9   train  \n",
      "10  train  \n",
      "11  train  \n",
      "12  train  \n",
      "13  train  \n",
      "14  train  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "INPUT_PATH = \"data/raw/RMBR4-2_export_test_1.csv\"\n",
    "OUTPUT_DIR = \"data/processed\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"RMBR4-2_export_processed.csv\")\n",
    "\n",
    "INTERVAL_SIZE = 10           # 10 work periods per detection interval\n",
    "TEST_SIZE = 0.2              # last 20% as test (time-aware split)\n",
    "FILL_METHOD = \"median\"       # options: \"median\", \"mean\", \"ffill\"\n",
    "\n",
    "FEATURE_COLS = [\"mean_value\", \"peak_value\"]\n",
    "TIME_COLS = [\"period_start_time\", \"period_end_time\"]\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_PATH}\")\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# =========================\n",
    "# Validate columns\n",
    "# =========================\n",
    "required_cols = [\"work_period\"] + FEATURE_COLS + TIME_COLS\n",
    "missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing required columns: {missing_cols}. Available: {df.columns.tolist()}\")\n",
    "\n",
    "# =========================\n",
    "# Parse times and sort (time-aware)\n",
    "# =========================\n",
    "for c in TIME_COLS:\n",
    "    df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Drop rows with invalid end time (critical)\n",
    "df = df.dropna(subset=[\"period_end_time\"]).copy()\n",
    "\n",
    "# Ensure numeric\n",
    "df[\"work_period\"] = pd.to_numeric(df[\"work_period\"], errors=\"coerce\")\n",
    "for c in FEATURE_COLS:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"work_period\"] + FEATURE_COLS).copy()\n",
    "\n",
    "# Sort by period end time (or you can sort by work_period)\n",
    "df = df.sort_values(\"period_end_time\").reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# Create interval_id (every 10 work periods)\n",
    "# =========================\n",
    "df[\"interval_id\"] = ((df[\"work_period\"] - 1) // INTERVAL_SIZE) + 1\n",
    "\n",
    "# =========================\n",
    "# Handle missing values (features)\n",
    "# =========================\n",
    "if FILL_METHOD == \"ffill\":\n",
    "    df[FEATURE_COLS] = df[FEATURE_COLS].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "elif FILL_METHOD == \"mean\":\n",
    "    df[FEATURE_COLS] = df[FEATURE_COLS].fillna(df[FEATURE_COLS].mean(numeric_only=True))\n",
    "else:  # median\n",
    "    df[FEATURE_COLS] = df[FEATURE_COLS].fillna(df[FEATURE_COLS].median(numeric_only=True))\n",
    "\n",
    "if df[FEATURE_COLS].isna().any().any():\n",
    "    raise ValueError(\"Missing values remain after filling. Check your input data.\")\n",
    "\n",
    "# =========================\n",
    "# Train/Test split (time-aware)\n",
    "# =========================\n",
    "n = len(df)\n",
    "if n < 10:\n",
    "    raise ValueError(f\"Not enough periods ({n}) to split reliably.\")\n",
    "\n",
    "split_idx = int(np.floor((1 - TEST_SIZE) * n))\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df  = df.iloc[split_idx:].copy()\n",
    "\n",
    "# =========================\n",
    "# Standardize features (fit on train, transform both)\n",
    "# =========================\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_df[FEATURE_COLS])\n",
    "test_scaled  = scaler.transform(test_df[FEATURE_COLS])\n",
    "\n",
    "for i, col in enumerate(FEATURE_COLS):\n",
    "    train_df[f\"{col}_z\"] = train_scaled[:, i]\n",
    "    test_df[f\"{col}_z\"]  = test_scaled[:, i]\n",
    "\n",
    "# Combine back\n",
    "processed_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "processed_df[\"split\"] = [\"train\"] * len(train_df) + [\"test\"] * len(test_df)\n",
    "\n",
    "# =========================\n",
    "# Save\n",
    "# =========================\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "processed_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved: {OUTPUT_PATH}\")\n",
    "print(f\"Train size: {len(train_df)} | Test size: {len(test_df)}\")\n",
    "print(processed_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53115440",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "Input: data/preprocessed/RMBR4-2_export_preprocessed.csv\n",
    "\n",
    "For each detection interval (interval_id), perform linear regression using:\n",
    "\n",
    "Independent variable (X): work_period\n",
    "\n",
    "Dependent variables (y): mean_value_z and peak_value_z\n",
    "\n",
    "Apply two implementations for comparison:\n",
    "\n",
    "From scratch (gradient descent): estimate theta0 and theta1\n",
    "\n",
    "scikit-learn: estimate theta0 and theta1\n",
    "\n",
    "Aggregate the results into a new table keyed by interval_id, with one row per interval.\n",
    "\n",
    "Output: data/models/interval_theta_table.csv\n",
    "\n",
    "Note: Each interval yields two sets of parameters (mean and peak), producing eight parameter columns:\n",
    "\n",
    "scratch_mean_theta0, scratch_mean_theta1\n",
    "\n",
    "scratch_peak_theta0, scratch_peak_theta1\n",
    "\n",
    "sklearn_mean_theta0, sklearn_mean_theta1\n",
    "\n",
    "sklearn_peak_theta0, sklearn_peak_theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59466701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved interval theta table to: data/models\\interval_theta_table_model_implement.csv\n",
      "    interval_id  n_points  scratch_mean_theta0  scratch_mean_theta1  \\\n",
      "0             1        10             0.177840            -0.102788   \n",
      "1             2        10            -0.840417             0.099498   \n",
      "2             3        10             0.812069            -0.030792   \n",
      "3             4        10             3.709370            -0.092001   \n",
      "4             5        10            -6.638421             0.143098   \n",
      "5             6        10            -7.841825             0.148661   \n",
      "6             7        10            -1.328246             0.025953   \n",
      "7             8        10             8.827536            -0.117293   \n",
      "8             9        10            -2.426644             0.025784   \n",
      "9            10        10           -12.771144             0.136762   \n",
      "10           11        10            -7.055246             0.066677   \n",
      "11           12        10             1.921729            -0.016855   \n",
      "12           13        10            23.568099            -0.186964   \n",
      "13           14        10            -2.187600             0.014235   \n",
      "14           15        10           -47.025468             0.314999   \n",
      "15           16        10            15.991617            -0.101265   \n",
      "16           17        10             6.346013            -0.035950   \n",
      "17           18        10           -12.710744             0.070611   \n",
      "18           19        10             0.048246             0.000552   \n",
      "19           20        10             8.129636            -0.041069   \n",
      "\n",
      "    scratch_peak_theta0  scratch_peak_theta1  sklearn_mean_theta0  \\\n",
      "0              0.258191            -0.154178             0.177840   \n",
      "1             -0.934766             0.073145            -0.840417   \n",
      "2              2.212510            -0.074561             0.812069   \n",
      "3              5.002872            -0.124509             3.709370   \n",
      "4             -5.626308             0.113802            -6.638421   \n",
      "5             -6.259266             0.120933            -7.841825   \n",
      "6             -3.004343             0.050370            -1.328246   \n",
      "7             14.857793            -0.190981             8.827536   \n",
      "8             -0.516137             0.003248            -2.426644   \n",
      "9            -11.533329             0.122986           -12.771144   \n",
      "10           -10.357815             0.095611            -7.055246   \n",
      "11            -4.215398             0.036083             1.921729   \n",
      "12            20.512937            -0.167809            23.568099   \n",
      "13           -10.595313             0.077543            -2.187600   \n",
      "14           -21.801136             0.145204           -47.025468   \n",
      "15            -3.562197             0.024850            15.991617   \n",
      "16             6.098584            -0.035543             6.346013   \n",
      "17           -15.181637             0.083932           -12.710744   \n",
      "18            16.522956            -0.086209             0.048246   \n",
      "19            -0.567414             0.002630             8.129636   \n",
      "\n",
      "    sklearn_mean_theta1  sklearn_peak_theta0  sklearn_peak_theta1  \n",
      "0             -0.102788             0.258191            -0.154178  \n",
      "1              0.099498            -0.934766             0.073145  \n",
      "2             -0.030792             2.212510            -0.074561  \n",
      "3             -0.092001             5.002872            -0.124509  \n",
      "4              0.143098            -5.626308             0.113802  \n",
      "5              0.148661            -6.259266             0.120933  \n",
      "6              0.025953            -3.004343             0.050370  \n",
      "7             -0.117293            14.857793            -0.190981  \n",
      "8              0.025784            -0.516137             0.003248  \n",
      "9              0.136762           -11.533329             0.122986  \n",
      "10             0.066677           -10.357815             0.095611  \n",
      "11            -0.016855            -4.215398             0.036083  \n",
      "12            -0.186964            20.512937            -0.167809  \n",
      "13             0.014235           -10.595313             0.077543  \n",
      "14             0.314999           -21.801136             0.145204  \n",
      "15            -0.101265            -3.562197             0.024850  \n",
      "16            -0.035950             6.098584            -0.035543  \n",
      "17             0.070611           -15.181637             0.083932  \n",
      "18             0.000552            16.522956            -0.086209  \n",
      "19            -0.041069            -0.567414             0.002630  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "INPUT_PATH = \"data/processed/RMBR4-2_export_processed.csv\"\n",
    "OUTPUT_DIR = \"data/models\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"interval_theta_table_model_implement.csv\")\n",
    "\n",
    "X_COL = \"work_period\"\n",
    "GROUP_COL = \"interval_id\"\n",
    "\n",
    "# Use standardized targets from preprocessing\n",
    "MEAN_TARGET = \"mean_value_z\"\n",
    "PEAK_TARGET = \"peak_value_z\"\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "ITERATIONS = 3000\n",
    "\n",
    "# =========================\n",
    "# From-scratch linear regression (1D) with gradient descent\n",
    "# =========================\n",
    "def hypothesis(theta0: float, theta1: float, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"h_theta(x) = theta0 + theta1 * x\"\"\"\n",
    "    return theta0 + theta1 * x\n",
    "\n",
    "def mse_cost(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Mean squared error (MSE)\"\"\"\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def gradient_descent_1d(x: np.ndarray, y: np.ndarray, lr: float, iters: int):\n",
    "    \"\"\"\n",
    "    Train theta0, theta1 using gradient descent on 1D linear regression.\n",
    "\n",
    "    Cost: J = (1/m) * sum((h(x_i) - y_i)^2)\n",
    "\n",
    "    Gradients:\n",
    "        dJ/dtheta0 = (2/m) * sum(h(x_i) - y_i)\n",
    "        dJ/dtheta1 = (2/m) * sum((h(x_i) - y_i) * x_i)\n",
    "\n",
    "    Updates:\n",
    "        theta0 := theta0 - lr * dJ/dtheta0\n",
    "        theta1 := theta1 - lr * dJ/dtheta1\n",
    "    \"\"\"\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "\n",
    "    m = len(x)\n",
    "    if m == 0:\n",
    "        raise ValueError(\"Empty training set for this interval.\")\n",
    "\n",
    "    theta0 = 0.0\n",
    "    theta1 = 0.0\n",
    "\n",
    "    for _ in range(iters):\n",
    "        y_pred = hypothesis(theta0, theta1, x)\n",
    "        error = y_pred - y\n",
    "\n",
    "        grad_theta0 = (2.0 / m) * np.sum(error)\n",
    "        grad_theta1 = (2.0 / m) * np.sum(error * x)\n",
    "\n",
    "        theta0 -= lr * grad_theta0\n",
    "        theta1 -= lr * grad_theta1\n",
    "\n",
    "    return float(theta0), float(theta1)\n",
    "\n",
    "def fit_scratch_with_centering(x: np.ndarray, y: np.ndarray, lr: float, iters: int):\n",
    "    \"\"\"\n",
    "    Optional x-centering improves GD stability.\n",
    "    Convert parameters back to original x scale after training.\n",
    "    \"\"\"\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "\n",
    "    mean_x = float(np.mean(x))\n",
    "    x_center = x - mean_x\n",
    "\n",
    "    theta0_c, theta1_c = gradient_descent_1d(x_center, y, lr=lr, iters=iters)\n",
    "\n",
    "    # y = theta0_c + theta1_c*(x - mean_x) = (theta0_c - theta1_c*mean_x) + theta1_c*x\n",
    "    theta0 = theta0_c - theta1_c * mean_x\n",
    "    theta1 = theta1_c\n",
    "\n",
    "    return float(theta0), float(theta1)\n",
    "\n",
    "# =========================\n",
    "# scikit-learn fit\n",
    "# =========================\n",
    "def fit_sklearn(x: np.ndarray, y: np.ndarray):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1, 1), y)\n",
    "    return float(model.intercept_), float(model.coef_[0])\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {INPUT_PATH}\")\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "required = [GROUP_COL, X_COL, MEAN_TARGET, PEAK_TARGET]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}. Available: {df.columns.tolist()}\")\n",
    "\n",
    "# Ensure numeric\n",
    "df[X_COL] = pd.to_numeric(df[X_COL], errors=\"coerce\")\n",
    "df[MEAN_TARGET] = pd.to_numeric(df[MEAN_TARGET], errors=\"coerce\")\n",
    "df[PEAK_TARGET] = pd.to_numeric(df[PEAK_TARGET], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=required).copy()\n",
    "df = df.sort_values([GROUP_COL, X_COL])\n",
    "\n",
    "rows = []\n",
    "\n",
    "for interval_id, g in df.groupby(GROUP_COL):\n",
    "    x = g[X_COL].to_numpy(dtype=float)\n",
    "\n",
    "    y_mean = g[MEAN_TARGET].to_numpy(dtype=float)\n",
    "    y_peak = g[PEAK_TARGET].to_numpy(dtype=float)\n",
    "\n",
    "    # ----- scratch -----\n",
    "    s_mean_theta0, s_mean_theta1 = fit_scratch_with_centering(x, y_mean, LEARNING_RATE, ITERATIONS)\n",
    "    s_peak_theta0, s_peak_theta1 = fit_scratch_with_centering(x, y_peak, LEARNING_RATE, ITERATIONS)\n",
    "\n",
    "    # ----- sklearn -----\n",
    "    k_mean_theta0, k_mean_theta1 = fit_sklearn(x, y_mean)\n",
    "    k_peak_theta0, k_peak_theta1 = fit_sklearn(x, y_peak)\n",
    "\n",
    "    rows.append({\n",
    "        \"interval_id\": interval_id,\n",
    "        \"n_points\": len(g),\n",
    "\n",
    "        \"scratch_mean_theta0\": s_mean_theta0,\n",
    "        \"scratch_mean_theta1\": s_mean_theta1,\n",
    "        \"scratch_peak_theta0\": s_peak_theta0,\n",
    "        \"scratch_peak_theta1\": s_peak_theta1,\n",
    "\n",
    "        \"sklearn_mean_theta0\": k_mean_theta0,\n",
    "        \"sklearn_mean_theta1\": k_mean_theta1,\n",
    "        \"sklearn_peak_theta0\": k_peak_theta0,\n",
    "        \"sklearn_peak_theta1\": k_peak_theta1,\n",
    "    })\n",
    "\n",
    "theta_table = pd.DataFrame(rows).sort_values(\"interval_id\").reset_index(drop=True)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "theta_table.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved interval theta table to: {OUTPUT_PATH}\")\n",
    "print(theta_table.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41ebb0",
   "metadata": {},
   "source": [
    "# Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf8a6e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluated table to: data/models\\interval_theta_table_model_evaluated.csv\n",
      "Plots saved to: data/models\\plots\n",
      "   interval_id  n_points  scratch_mean_theta0  scratch_mean_theta1  \\\n",
      "0            1        10             0.177840            -0.102788   \n",
      "1            2        10            -0.840417             0.099498   \n",
      "2            3        10             0.812069            -0.030792   \n",
      "3            4        10             3.709370            -0.092001   \n",
      "4            5        10            -6.638421             0.143098   \n",
      "\n",
      "   scratch_peak_theta0  scratch_peak_theta1  sklearn_mean_theta0  \\\n",
      "0             0.258191            -0.154178             0.177840   \n",
      "1            -0.934766             0.073145            -0.840417   \n",
      "2             2.212510            -0.074561             0.812069   \n",
      "3             5.002872            -0.124509             3.709370   \n",
      "4            -5.626308             0.113802            -6.638421   \n",
      "\n",
      "   sklearn_mean_theta1  sklearn_peak_theta0  sklearn_peak_theta1  ...  \\\n",
      "0            -0.102788             0.258191            -0.154178  ...   \n",
      "1             0.099498            -0.934766             0.073145  ...   \n",
      "2            -0.030792             2.212510            -0.074561  ...   \n",
      "3            -0.092001             5.002872            -0.124509  ...   \n",
      "4             0.143098            -5.626308             0.113802  ...   \n",
      "\n",
      "   scratch_mean_r2  sklearn_mean_rmse  sklearn_mean_mae  sklearn_mean_r2  \\\n",
      "0         0.104100           0.866113          0.723920         0.104100   \n",
      "1         0.110413           0.811192          0.619683         0.110413   \n",
      "2         0.014137           0.738582          0.637681         0.014137   \n",
      "3         0.066141           0.992950          0.794906         0.066141   \n",
      "4         0.222517           0.768287          0.683670         0.222517   \n",
      "\n",
      "   scratch_peak_rmse  scratch_peak_mae  scratch_peak_r2  sklearn_peak_rmse  \\\n",
      "0           0.663526          0.542385         0.308167           0.663526   \n",
      "1           0.511157          0.425356         0.144521           0.511157   \n",
      "2           0.848767          0.715812         0.059854           0.848767   \n",
      "3           1.298883          1.042511         0.070466           1.298883   \n",
      "4           0.653692          0.555041         0.200026           0.653692   \n",
      "\n",
      "   sklearn_peak_mae  sklearn_peak_r2  \n",
      "0          0.542385         0.308167  \n",
      "1          0.425356         0.144521  \n",
      "2          0.715812         0.059854  \n",
      "3          1.042511         0.070466  \n",
      "4          0.555041         0.200026  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "DATA_PATH = \"data/processed/RMBR4-2_export_processed.csv\"\n",
    "THETA_PATH = \"data/models/interval_theta_table_model_implement.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"data/models\"\n",
    "PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"interval_theta_table_model_evaluated.csv\")\n",
    "\n",
    "X_COL = \"work_period\"\n",
    "GROUP_COL = \"interval_id\"\n",
    "\n",
    "MEAN_TARGET = \"mean_value_z\"\n",
    "PEAK_TARGET = \"peak_value_z\"\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def predict(theta0, theta1, x):\n",
    "    \"\"\"h_theta(x) = theta0 + theta1 * x\"\"\"\n",
    "    return theta0 + theta1 * x\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def plot_regression(x, y, y_scratch, y_sklearn, interval_id, target_name):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(x, y, label=\"Data\", color=\"black\")\n",
    "    plt.plot(x, y_scratch, label=\"Scratch\", linewidth=2)\n",
    "    plt.plot(x, y_sklearn, label=\"Sklearn\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Work Period\")\n",
    "    plt.ylabel(target_name)\n",
    "    plt.title(f\"Interval {interval_id} â€“ {target_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = f\"interval_{interval_id}_{target_name}.png\"\n",
    "    plt.savefig(os.path.join(PLOT_DIR, fname))\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "theta_df = pd.read_csv(THETA_PATH)\n",
    "\n",
    "required_cols = [GROUP_COL, X_COL, MEAN_TARGET, PEAK_TARGET]\n",
    "df = df.dropna(subset=required_cols).copy()\n",
    "\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "# =========================\n",
    "# Evaluation per interval\n",
    "# =========================\n",
    "for _, row in theta_df.iterrows():\n",
    "    interval_id = row[\"interval_id\"]\n",
    "\n",
    "    g = df[df[GROUP_COL] == interval_id].sort_values(X_COL)\n",
    "    if len(g) < 2:\n",
    "        continue\n",
    "\n",
    "    x = g[X_COL].to_numpy(dtype=float)\n",
    "\n",
    "    # ----- MEAN -----\n",
    "    y_mean = g[MEAN_TARGET].to_numpy(dtype=float)\n",
    "\n",
    "    y_mean_scratch = predict(\n",
    "        row[\"scratch_mean_theta0\"],\n",
    "        row[\"scratch_mean_theta1\"],\n",
    "        x\n",
    "    )\n",
    "    y_mean_sklearn = predict(\n",
    "        row[\"sklearn_mean_theta0\"],\n",
    "        row[\"sklearn_mean_theta1\"],\n",
    "        x\n",
    "    )\n",
    "\n",
    "    mean_metrics_scratch = compute_metrics(y_mean, y_mean_scratch)\n",
    "    mean_metrics_sklearn = compute_metrics(y_mean, y_mean_sklearn)\n",
    "\n",
    "    plot_regression(\n",
    "        x, y_mean,\n",
    "        y_mean_scratch, y_mean_sklearn,\n",
    "        interval_id, \"mean\"\n",
    "    )\n",
    "\n",
    "    # ----- PEAK -----\n",
    "    y_peak = g[PEAK_TARGET].to_numpy(dtype=float)\n",
    "\n",
    "    y_peak_scratch = predict(\n",
    "        row[\"scratch_peak_theta0\"],\n",
    "        row[\"scratch_peak_theta1\"],\n",
    "        x\n",
    "    )\n",
    "    y_peak_sklearn = predict(\n",
    "        row[\"sklearn_peak_theta0\"],\n",
    "        row[\"sklearn_peak_theta1\"],\n",
    "        x\n",
    "    )\n",
    "\n",
    "    peak_metrics_scratch = compute_metrics(y_peak, y_peak_scratch)\n",
    "    peak_metrics_sklearn = compute_metrics(y_peak, y_peak_sklearn)\n",
    "\n",
    "    plot_regression(\n",
    "        x, y_peak,\n",
    "        y_peak_scratch, y_peak_sklearn,\n",
    "        interval_id, \"peak\"\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"interval_id\": interval_id,\n",
    "\n",
    "        # Mean â€“ scratch\n",
    "        \"scratch_mean_rmse\": mean_metrics_scratch[\"rmse\"],\n",
    "        \"scratch_mean_mae\": mean_metrics_scratch[\"mae\"],\n",
    "        \"scratch_mean_r2\": mean_metrics_scratch[\"r2\"],\n",
    "\n",
    "        # Mean â€“ sklearn\n",
    "        \"sklearn_mean_rmse\": mean_metrics_sklearn[\"rmse\"],\n",
    "        \"sklearn_mean_mae\": mean_metrics_sklearn[\"mae\"],\n",
    "        \"sklearn_mean_r2\": mean_metrics_sklearn[\"r2\"],\n",
    "\n",
    "        # Peak â€“ scratch\n",
    "        \"scratch_peak_rmse\": peak_metrics_scratch[\"rmse\"],\n",
    "        \"scratch_peak_mae\": peak_metrics_scratch[\"mae\"],\n",
    "        \"scratch_peak_r2\": peak_metrics_scratch[\"r2\"],\n",
    "\n",
    "        # Peak â€“ sklearn\n",
    "        \"sklearn_peak_rmse\": peak_metrics_sklearn[\"rmse\"],\n",
    "        \"sklearn_peak_mae\": peak_metrics_sklearn[\"mae\"],\n",
    "        \"sklearn_peak_r2\": peak_metrics_sklearn[\"r2\"],\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# Merge metrics into theta table\n",
    "# =========================\n",
    "metrics_df = pd.DataFrame(results)\n",
    "final_df = theta_df.merge(metrics_df, on=\"interval_id\", how=\"left\")\n",
    "\n",
    "final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved evaluated table to: {OUTPUT_PATH}\")\n",
    "print(f\"Plots saved to: {PLOT_DIR}\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18059830",
   "metadata": {},
   "source": [
    "### ðŸ”¹ After Session 1 (Homework)\n",
    "\n",
    "- Refactor your notebook into **modular Python scripts**:  \n",
    "  - `data_loader.py` â€“ functions to load data from CSV, API, and DB.  \n",
    "  - `preprocessing.py` â€“ cleaning, normalization, train/test split.  \n",
    "  - `model.py` â€“ regression model implementations.  \n",
    "  - `evaluation.py` â€“ metrics, plots, reporting.  \n",
    "- Ensure each module can run independently.  \n",
    "\n",
    "ðŸ’¡ This will prepare you for **Session 2 (MLOps)**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce0f6c",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Before Session 2: Preparing for MLOps\n",
    "\n",
    "- Replicate the structure, files and resources that you developed during the **DataStreamVisualization_Workshop**\n",
    "- Use it to organize this project into a folder structure like:\n",
    "\n",
    "```txt\n",
    "linear_regression_project/\n",
    "â”‚â”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ raw/\n",
    "â”‚   â”œâ”€â”€ processed/\n",
    "â”‚â”€â”€ notebooks/\n",
    "â”‚   â”œâ”€â”€ EDA.ipynb\n",
    "â”‚   â”œâ”€â”€ linear_regression.ipynb\n",
    "â”‚â”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ data_loader.py\n",
    "â”‚   â”œâ”€â”€ preprocessing.py\n",
    "â”‚   â”œâ”€â”€ model.py\n",
    "â”‚   â”œâ”€â”€ evaluation.py\n",
    "â”‚â”€â”€ configs/\n",
    "â”‚   â”œâ”€â”€ experiment_config.yaml\n",
    "â”‚â”€â”€ experiments/\n",
    "â”‚   â”œâ”€â”€ results.csv\n",
    "â”‚â”€â”€ requirements.txt\n",
    "â”‚â”€â”€ README.md\n",
    "````\n",
    "\n",
    "* Create a **YAML config file** with parameters:\n",
    "\n",
    "  * Data source path/API endpoint/DB connection string\n",
    "  * Learning rate, iterations, train/test split ratio\n",
    "  * Feature to use as predictor\n",
    "\n",
    "* Document how to run your scripts step-by-step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84406ab3",
   "metadata": {},
   "source": [
    "### ðŸ”¹ During Session 2: MLOps Architecture\n",
    "\n",
    "* Apply the **Robot PM MLOps design patterns**:\n",
    "\n",
    "  * **Separation of concerns**: Each module is independent.\n",
    "  * **Configuration-driven**: Experiments are parameterized by configs, not hard-coded values.\n",
    "  * **Experiment tracking**: Save model performance metrics in `experiments/results.csv`.\n",
    "  * **Reproducibility**: Ensure anyone can re-run your experiment with the same results.\n",
    "\n",
    "* Discuss:\n",
    "\n",
    "  * Why modularity matters for ML projects.\n",
    "  * How config management avoids errors in scaling ML experiments.\n",
    "  * How this workflow connects to real-world ML pipelines.\n",
    "\n",
    "ðŸ’¡ **Deliverable during Session 2**:\n",
    "\n",
    "* A structured project with modular code, configs, and experiment tracking.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb92881",
   "metadata": {},
   "source": [
    "### Alert Design Based on Raw Robot Force Data\n",
    "\n",
    "Based on the raw robot force data aggregated at the work-period level, alert thresholds are designed for both mean force and peak force to identify potential system degradation and failure risks.\n",
    "\n",
    "The analysis is performed over fixed detection intervals, each consisting of a predefined number of consecutive work periods.\n",
    "\n",
    "Alert Logic\n",
    "\n",
    "For each detection interval, a linear regression model is fitted using the work period index as the independent variable. The absolute value of the regression slope is used to quantify the trend strength within the interval.\n",
    "\n",
    "Trend-based Trigger Condition\n",
    "\n",
    "If the absolute value of the slope within a detection interval exceeds a predefined slope threshold, the interval is flagged for further inspection.\n",
    "\n",
    "This condition indicates an abnormal increasing or decreasing trend in the force signal over time.\n",
    "\n",
    "Aging Alert (Mean Force)\n",
    "\n",
    "Within the start and end time of the detection interval, if:\n",
    "\n",
    "the absolute slope of the mean force exceeds the slope threshold, and\n",
    "\n",
    "the average mean force within the interval exceeds the designed mean force alert threshold,\n",
    "\n",
    "then an aging alert is raised.\n",
    "\n",
    "This alert suggests that the system may be experiencing gradual degradation and may require maintenance.\n",
    "\n",
    "Failure Alert (Peak Force)\n",
    "\n",
    "Within the start and end time of the detection interval, if:\n",
    "\n",
    "the absolute slope of the peak force exceeds the slope threshold, and\n",
    "\n",
    "the maximum peak force within the interval exceeds the designed peak force alert threshold,\n",
    "\n",
    "then a fault alert is raised.\n",
    "\n",
    "This alert indicates the possibility of an imminent or acute system failure.\n",
    "\n",
    "Interpretation\n",
    "\n",
    "Mean force trends are used to capture long-term structural changes and gradual wear, making them suitable indicators for system aging.\n",
    "\n",
    "Peak force trends are more sensitive to sudden stress or abnormal operating conditions and are therefore used to detect potential faults.\n",
    "\n",
    "By combining trend-based detection with level-based thresholds, the alerting mechanism reduces false positives while maintaining sensitivity to meaningful system changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba65362",
   "metadata": {},
   "source": [
    "## Rationale for Selecting Mean and Peak Alert Thresholds\n",
    "\n",
    "The alert thresholds for mean force and peak force are derived directly from the raw work-period level data to ensure data-driven and robust decision boundaries.\n",
    "\n",
    "Mean Force Alert Threshold\n",
    "\n",
    "The mean force alert threshold is designed to represent the upper bound of normal operating behavior under typical working conditions.\n",
    "\n",
    "The threshold is calculated using a robust statistical approach based on the median and the Median Absolute Deviation (MAD):\n",
    "\n",
    "Mean Alert Threshold\n",
    "=\n",
    "median(mean_value)\n",
    "+\n",
    "ð‘˜\n",
    "Ã—\n",
    "1.4826\n",
    "Ã—\n",
    "MAD(mean_value)\n",
    "Mean Alert Threshold=median(mean_value)+kÃ—1.4826Ã—MAD(mean_value)\n",
    "\n",
    "This method is preferred over mean and standard deviation because:\n",
    "\n",
    "Robot force data often exhibit skewed distributions and outliers.\n",
    "\n",
    "The median and MAD are less sensitive to extreme values, providing a more stable and reliable threshold.\n",
    "\n",
    "The resulting threshold reflects the typical variability of the system rather than being dominated by rare extreme events.\n",
    "\n",
    "When the average mean force within a detection interval exceeds this threshold, it suggests progressive mechanical wear or system aging, which is typically characterized by sustained increases in average load.\n",
    "\n",
    "Peak Force Alert Threshold\n",
    "\n",
    "The peak force alert threshold is designed to detect abnormal transient loads that may indicate imminent or sudden system failures.\n",
    "\n",
    "The threshold is calculated using the same robust statistical framework:\n",
    "\n",
    "Peak Alert Threshold\n",
    "=\n",
    "median(peak_value)\n",
    "+\n",
    "ð‘˜\n",
    "Ã—\n",
    "1.4826\n",
    "Ã—\n",
    "MAD(peak_value)\n",
    "Peak Alert Threshold=median(peak_value)+kÃ—1.4826Ã—MAD(peak_value)\n",
    "\n",
    "This approach is appropriate because:\n",
    "\n",
    "Peak force signals are inherently more volatile and prone to sharp spikes.\n",
    "\n",
    "Robust statistics prevent a small number of extreme spikes from inflating the threshold.\n",
    "\n",
    "The threshold captures the upper range of normal transient behavior while remaining sensitive to abnormal stress events.\n",
    "\n",
    "Exceeding the peak force alert threshold within a detection interval indicates a potential fault condition, such as mechanical jamming, collision, or actuator malfunction.\n",
    "\n",
    "Choice of the Scaling Factor \n",
    "ð‘˜\n",
    "k\n",
    "\n",
    "The scaling factor \n",
    "ð‘˜\n",
    "k controls the strictness of the alert thresholds.\n",
    "\n",
    "A typical value of \n",
    "ð‘˜\n",
    "=\n",
    "3.5\n",
    "k=3.5 is selected to:\n",
    "\n",
    "Minimize false positives under normal operating conditions.\n",
    "\n",
    "Preserve sensitivity to genuinely abnormal behavior.\n",
    "\n",
    "This choice is consistent with established practices in robust outlier detection and industrial condition monitoring.\n",
    "\n",
    "Summary\n",
    "\n",
    "Mean force thresholds target long-term degradation and aging effects.\n",
    "\n",
    "Peak force thresholds target short-term abnormal stress and fault conditions.\n",
    "\n",
    "Both thresholds are data-driven, robust, and interpretable, making them suitable for real-world predictive maintenance applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a66a1425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved experiment tracking results to: experiments/results.csv\n",
      "\n",
      "--- Thresholds used ---\n",
      "{'K': 3.5, 'slope_source': 'sklearn', 'mean_alert_threshold': 4.310588304459788, 'peak_alert_threshold': 10.972072798187499, 'mean_slope_threshold': 0.28998019526948016, 'peak_slope_threshold': 0.29888605648519484}\n",
      "\n",
      "--- Preview results ---\n",
      "    interval_id              interval_start_time  \\\n",
      "0             1 2022-10-17 12:19:22.005000+00:00   \n",
      "1             2 2022-10-17 12:35:51.838000+00:00   \n",
      "2             3 2022-10-17 13:08:22.832000+00:00   \n",
      "3             4 2022-10-17 13:20:15.677000+00:00   \n",
      "4             5 2022-10-17 13:47:28.226000+00:00   \n",
      "5             6 2022-10-17 13:56:36.935000+00:00   \n",
      "6             7 2022-10-17 14:07:11.332000+00:00   \n",
      "7             8 2022-10-17 14:16:34.799000+00:00   \n",
      "8             9 2022-10-17 14:26:26.237000+00:00   \n",
      "9            10 2022-10-17 14:36:42.017000+00:00   \n",
      "10           11 2022-10-17 14:47:08.972000+00:00   \n",
      "11           12 2022-10-17 14:55:59.959000+00:00   \n",
      "12           13 2022-10-17 15:09:10.982000+00:00   \n",
      "13           14 2022-10-17 15:20:44.942000+00:00   \n",
      "14           15 2022-10-17 15:31:17.557000+00:00   \n",
      "15           16 2022-10-17 17:36:52.252000+00:00   \n",
      "16           17 2022-10-17 17:57:44.683000+00:00   \n",
      "17           18 2022-10-17 18:08:21.896000+00:00   \n",
      "18           19 2022-10-17 18:17:34.930000+00:00   \n",
      "19           20 2022-10-17 18:27:22.355000+00:00   \n",
      "\n",
      "                  interval_end_time           predicted_failure_time  \\\n",
      "0  2022-10-17 12:35:31.262000+00:00 2022-10-31 12:35:31.262000+00:00   \n",
      "1  2022-10-17 13:08:11.543000+00:00 2022-10-31 13:08:11.543000+00:00   \n",
      "2  2022-10-17 13:19:45.820000+00:00 2022-10-31 13:19:45.820000+00:00   \n",
      "3  2022-10-17 13:47:09.395000+00:00 2022-10-31 13:47:09.395000+00:00   \n",
      "4  2022-10-17 13:56:14.190000+00:00 2022-10-31 13:56:14.190000+00:00   \n",
      "5  2022-10-17 14:06:54.378000+00:00 2022-10-31 14:06:54.378000+00:00   \n",
      "6  2022-10-17 14:16:15.852000+00:00 2022-10-31 14:16:15.852000+00:00   \n",
      "7  2022-10-17 14:26:07.097000+00:00 2022-10-31 14:26:07.097000+00:00   \n",
      "8  2022-10-17 14:36:23.096000+00:00 2022-10-31 14:36:23.096000+00:00   \n",
      "9  2022-10-17 14:46:50.223000+00:00 2022-10-31 14:46:50.223000+00:00   \n",
      "10 2022-10-17 14:55:30.041000+00:00 2022-10-31 14:55:30.041000+00:00   \n",
      "11 2022-10-17 15:08:54.306000+00:00 2022-10-31 15:08:54.306000+00:00   \n",
      "12 2022-10-17 15:20:31.831000+00:00 2022-10-31 15:20:31.831000+00:00   \n",
      "13 2022-10-17 15:30:33.878000+00:00 2022-10-31 15:30:33.878000+00:00   \n",
      "14 2022-10-17 17:33:41.659000+00:00 2022-10-31 17:33:41.659000+00:00   \n",
      "15 2022-10-17 17:57:25.819000+00:00 2022-10-31 17:57:25.819000+00:00   \n",
      "16 2022-10-17 18:08:05.003000+00:00 2022-10-31 18:08:05.003000+00:00   \n",
      "17 2022-10-17 18:17:19.753000+00:00 2022-10-31 18:17:19.753000+00:00   \n",
      "18 2022-10-17 18:27:01.518000+00:00 2022-10-31 18:27:01.518000+00:00   \n",
      "19 2022-10-17 18:36:03.880000+00:00 2022-10-31 18:36:03.880000+00:00   \n",
      "\n",
      "   failure_type alert_reason  \n",
      "0      No Alert    No alert.  \n",
      "1      No Alert    No alert.  \n",
      "2      No Alert    No alert.  \n",
      "3      No Alert    No alert.  \n",
      "4      No Alert    No alert.  \n",
      "5      No Alert    No alert.  \n",
      "6      No Alert    No alert.  \n",
      "7      No Alert    No alert.  \n",
      "8      No Alert    No alert.  \n",
      "9      No Alert    No alert.  \n",
      "10     No Alert    No alert.  \n",
      "11     No Alert    No alert.  \n",
      "12     No Alert    No alert.  \n",
      "13     No Alert    No alert.  \n",
      "14     No Alert    No alert.  \n",
      "15     No Alert    No alert.  \n",
      "16     No Alert    No alert.  \n",
      "17     No Alert    No alert.  \n",
      "18     No Alert    No alert.  \n",
      "19     No Alert    No alert.  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "PERIOD_PATH = \"data/raw/RMBR4-2_export_test_1.csv\"              # period-level summary with times\n",
    "THETA_PATH  = \"data/models/interval_theta_table_model_implement.csv\"           # implement theta table (per interval)\n",
    "\n",
    "OUTPUT_RESULTS_PATH = \"experiments/results.csv\"\n",
    "\n",
    "INTERVAL_SIZE = 10\n",
    "K = 3.5\n",
    "PREDICT_OFFSET_DAYS = 14\n",
    "\n",
    "# Choose which slope source to use: \"sklearn\" or \"scratch\"\n",
    "SLOPE_SOURCE = \"sklearn\"\n",
    "\n",
    "# =========================\n",
    "# Robust threshold helpers\n",
    "# =========================\n",
    "def robust_location_scale(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns (median, robust_sigma) where robust_sigma ~= std using MAD.\n",
    "    robust_sigma = 1.4826 * MAD\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if len(x) == 0:\n",
    "        return np.nan, np.nan\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med))\n",
    "    robust_sigma = 1.4826 * mad\n",
    "    return float(med), float(robust_sigma)\n",
    "\n",
    "def robust_threshold(x: np.ndarray, k: float):\n",
    "    med, rs = robust_location_scale(x)\n",
    "    if not np.isfinite(med) or not np.isfinite(rs):\n",
    "        return np.nan\n",
    "    return float(med + k * rs)\n",
    "\n",
    "# =========================\n",
    "# Load period-level data\n",
    "# =========================\n",
    "if not os.path.exists(PERIOD_PATH):\n",
    "    raise FileNotFoundError(f\"Period file not found: {PERIOD_PATH}\")\n",
    "if not os.path.exists(THETA_PATH):\n",
    "    raise FileNotFoundError(f\"Theta file not found: {THETA_PATH}\")\n",
    "\n",
    "period_df = pd.read_csv(PERIOD_PATH)\n",
    "\n",
    "required_period_cols = [\n",
    "    \"work_period\", \"mean_value\", \"peak_value\", \"period_start_time\", \"period_end_time\"\n",
    "]\n",
    "missing = [c for c in required_period_cols if c not in period_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in period file: {missing}\\nAvailable: {period_df.columns.tolist()}\")\n",
    "\n",
    "period_df[\"work_period\"] = pd.to_numeric(period_df[\"work_period\"], errors=\"coerce\")\n",
    "period_df[\"mean_value\"]  = pd.to_numeric(period_df[\"mean_value\"], errors=\"coerce\")\n",
    "period_df[\"peak_value\"]  = pd.to_numeric(period_df[\"peak_value\"], errors=\"coerce\")\n",
    "period_df[\"period_start_time\"] = pd.to_datetime(period_df[\"period_start_time\"], errors=\"coerce\", utc=True)\n",
    "period_df[\"period_end_time\"]   = pd.to_datetime(period_df[\"period_end_time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "period_df = period_df.dropna(subset=[\"work_period\", \"mean_value\", \"peak_value\", \"period_end_time\"]).copy()\n",
    "period_df = period_df.sort_values(\"work_period\").reset_index(drop=True)\n",
    "\n",
    "# Build interval_id from work_period\n",
    "period_df[\"interval_id\"] = ((period_df[\"work_period\"] - 1) // INTERVAL_SIZE) + 1\n",
    "\n",
    "# =========================\n",
    "# Build interval summary (start/end time + levels)\n",
    "# =========================\n",
    "interval_summary = (\n",
    "    period_df.groupby(\"interval_id\")\n",
    "    .agg(\n",
    "        interval_start_time=(\"period_start_time\", \"min\"),\n",
    "        interval_end_time=(\"period_end_time\", \"max\"),\n",
    "        interval_mean_level=(\"mean_value\", \"mean\"),  # average mean over periods in interval\n",
    "        interval_peak_level=(\"peak_value\", \"max\"),   # max peak over periods in interval\n",
    "        n_periods=(\"work_period\", \"count\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"interval_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Load theta table (implement output)\n",
    "# =========================\n",
    "theta_df = pd.read_csv(THETA_PATH)\n",
    "\n",
    "if SLOPE_SOURCE == \"scratch\":\n",
    "    mean_slope_col = \"scratch_mean_theta1\"\n",
    "    peak_slope_col = \"scratch_peak_theta1\"\n",
    "else:\n",
    "    mean_slope_col = \"sklearn_mean_theta1\"\n",
    "    peak_slope_col = \"sklearn_peak_theta1\"\n",
    "\n",
    "required_theta_cols = [\"interval_id\", mean_slope_col, peak_slope_col]\n",
    "missing = [c for c in required_theta_cols if c not in theta_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in theta file: {missing}\\nAvailable: {theta_df.columns.tolist()}\")\n",
    "\n",
    "theta_df[\"interval_id\"] = pd.to_numeric(theta_df[\"interval_id\"], errors=\"coerce\")\n",
    "theta_df[mean_slope_col] = pd.to_numeric(theta_df[mean_slope_col], errors=\"coerce\")\n",
    "theta_df[peak_slope_col] = pd.to_numeric(theta_df[peak_slope_col], errors=\"coerce\")\n",
    "theta_df = theta_df.dropna(subset=[\"interval_id\"]).copy()\n",
    "\n",
    "# =========================\n",
    "# Threshold design (from raw period-level data + theta table)\n",
    "# =========================\n",
    "# Level thresholds from raw period-level distributions\n",
    "mean_alert_threshold = robust_threshold(period_df[\"mean_value\"].to_numpy(), K)\n",
    "peak_alert_threshold = robust_threshold(period_df[\"peak_value\"].to_numpy(), K)\n",
    "\n",
    "# Slope thresholds from observed slopes across intervals\n",
    "mean_slope_threshold = robust_threshold(np.abs(theta_df[mean_slope_col].to_numpy()), K)\n",
    "peak_slope_threshold = robust_threshold(np.abs(theta_df[peak_slope_col].to_numpy()), K)\n",
    "\n",
    "# =========================\n",
    "# Merge interval_summary with slopes\n",
    "# =========================\n",
    "merged = interval_summary.merge(\n",
    "    theta_df[[\"interval_id\", mean_slope_col, peak_slope_col]],\n",
    "    on=\"interval_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Alert logic\n",
    "# =========================\n",
    "merged[\"mean_slope_flag\"] = merged[mean_slope_col].abs() > mean_slope_threshold\n",
    "merged[\"peak_slope_flag\"] = merged[peak_slope_col].abs() > peak_slope_threshold\n",
    "\n",
    "merged[\"aging_alert\"] = merged[\"mean_slope_flag\"] & (merged[\"interval_mean_level\"] > mean_alert_threshold)\n",
    "merged[\"fault_alert\"] = merged[\"peak_slope_flag\"] & (merged[\"interval_peak_level\"] > peak_alert_threshold)\n",
    "\n",
    "def failure_type(row):\n",
    "    if row[\"aging_alert\"] and row[\"fault_alert\"]:\n",
    "        return \"Aging + Fault\"\n",
    "    if row[\"fault_alert\"]:\n",
    "        return \"Fault\"\n",
    "    if row[\"aging_alert\"]:\n",
    "        return \"Aging\"\n",
    "    return \"No Alert\"\n",
    "\n",
    "merged[\"failure_type\"] = merged.apply(failure_type, axis=1)\n",
    "\n",
    "# Predicted failure time = interval end time + 14 days\n",
    "merged[\"predicted_failure_time\"] = merged[\"interval_end_time\"] + pd.Timedelta(days=PREDICT_OFFSET_DAYS)\n",
    "\n",
    "# Optional: add a human-readable reason (good for reports)\n",
    "def build_reason(r):\n",
    "    reasons = []\n",
    "    if r[\"aging_alert\"]:\n",
    "        reasons.append(\"Possible system aging: mean slope & mean level exceed thresholds.\")\n",
    "    if r[\"fault_alert\"]:\n",
    "        reasons.append(\"Possible failure: peak slope & peak level exceed thresholds.\")\n",
    "    return \" | \".join(reasons) if reasons else \"No alert.\"\n",
    "\n",
    "merged[\"alert_reason\"] = merged.apply(build_reason, axis=1)\n",
    "\n",
    "# =========================\n",
    "# Build and save experiments/results.csv\n",
    "# =========================\n",
    "results_df = merged[[\n",
    "    \"interval_id\",\n",
    "    \"interval_start_time\",\n",
    "    \"interval_end_time\",\n",
    "    \"predicted_failure_time\",\n",
    "    \"failure_type\",\n",
    "    \"alert_reason\",\n",
    "]].sort_values(\"interval_id\").reset_index(drop=True)\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_RESULTS_PATH), exist_ok=True)\n",
    "results_df.to_csv(OUTPUT_RESULTS_PATH, index=False)\n",
    "\n",
    "print(f\"Saved experiment tracking results to: {OUTPUT_RESULTS_PATH}\")\n",
    "print(\"\\n--- Thresholds used ---\")\n",
    "print({\n",
    "    \"K\": K,\n",
    "    \"slope_source\": SLOPE_SOURCE,\n",
    "    \"mean_alert_threshold\": mean_alert_threshold,\n",
    "    \"peak_alert_threshold\": peak_alert_threshold,\n",
    "    \"mean_slope_threshold\": mean_slope_threshold,\n",
    "    \"peak_slope_threshold\": peak_slope_threshold,\n",
    "})\n",
    "print(\"\\n--- Preview results ---\")\n",
    "print(results_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a145dabc",
   "metadata": {},
   "source": [
    "### ðŸ”¹ After Session 2: Extension & Homework\n",
    "\n",
    "0. **Submission Format**  \n",
    "   - This activity is **to be submitted individually**. Each student must create and manage their own project repository.\n",
    "\n",
    "1. **Workshop Replication**  \n",
    "   - This workshop is modeled on the structure, files, and resources used in the **DataStreamVisualization_Workshop**.  \n",
    "   - Your submission must replicate this style of organization and completeness.  \n",
    "\n",
    "2. **Repository Submission Instructions**  \n",
    "   - Create a **remote Git repository** named:  \n",
    "     ```\n",
    "     LinearRegressionArchitecture_Workshop\n",
    "     ```\n",
    "   - Once your repository is ready, send your instructor an email with the subject line:  \n",
    "     ```\n",
    "     Linear Regression Architecture Workshop\n",
    "     ```\n",
    "   - In the body of the email, paste the **full URL of your repository**, making sure it ends with the `.git` extension.  \n",
    "     - âœ… Correct example: `https://github.com/username/LinearRegressionArchitecture_Workshop.git`  \n",
    "     - âŒ Incorrect example: `https://github.com/username/LinearRegressionArchitecture_Workshop`\n",
    "\n",
    "3. **Repository Requirements**  \n",
    "   Your repository must contain:  \n",
    "   - A **frozen version of the codebase** (no further modifications after submission).  \n",
    "   - A `requirements.txt` file that lists all dependencies required to run your project.  \n",
    "   - A `README.md` file that:  \n",
    "     - Displays the title: **Linear Regression Architecture Workshop**.  \n",
    "     - Describes the work completed in the workshop.  \n",
    "     - Summarizes key design decisions.  \n",
    "\n",
    "4. **Notebook Updates (RobotPM_MLOps.ipynb)**  \n",
    "   - Open the notebook `RobotPM_MLOps.ipynb`.  \n",
    "   - Update it so that it highlights all changes made to the original project architecture and files.  \n",
    "   - Specifically, reference the lists provided in the notebook:  \n",
    "     - **Recommended Additions**  \n",
    "     - **Recommended Enhancements**  \n",
    "     - **Breakdown examples** (from both design breakdown sections).  \n",
    "\n",
    "5. **Expectations for Notebook Updates**  \n",
    "   - You are **not required to fully implement** the changes and updates at this stage.  \n",
    "   - Instead, create all **placeholders, stubs, and structure** needed to prepare the project for a future code review.  \n",
    "   - Think of this as **project scaffolding** for the upcoming implementation sprint cycle, which will be executed in a future project.  \n",
    "\n",
    "ðŸ’¡ **Final Deliverable**:  \n",
    "- A complete GitHub repository named `LinearRegressionArchitecture_Workshop` with the required structure, files, and documentation.  \n",
    "- An updated `RobotPM_MLOps.ipynb` notebook showing how the project architecture was extended and prepared for enhancements.  \n",
    "- Email submission to the instructor containing the `.git` repository URL.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
